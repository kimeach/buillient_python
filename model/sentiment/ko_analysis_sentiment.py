import torch
from transformers import BertForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer
from keybert import KeyBERT
import re

# CUDA ë””ë²„ê¹… í™˜ê²½ë³€ìˆ˜ ì„¤ì •
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()

# === 1. ê°ì„± ë¶„ì„ ëª¨ë¸ (monologg/kobert) ===
print("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
sentiment_model_name = "monologg/kobert"
sentiment_tokenizer = AutoTokenizer.from_pretrained(
    sentiment_model_name, 
    trust_remote_code=True,
    use_fast=False  # ì•ˆì •ì„±ì„ ìœ„í•´ fast tokenizer ë¹„í™œì„±í™”
)
sentiment_model = BertForSequenceClassification.from_pretrained(
    sentiment_model_name, 
    num_labels=3
)

# GPU ì‚¬ìš© ì‹œ ëª¨ë¸ì„ GPUë¡œ ì´ë™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
sentiment_model = sentiment_model.to(device)
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# ê°ì„± ì˜ˆì¸¡ í•¨ìˆ˜
def predict_sentiment(text):
    try:
        sentiment_model.eval()
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - íŠ¹ìˆ˜ë¬¸ì ë° ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = text.strip()
        
        if not text:
            return {"ì˜ˆì¸¡": "ì¤‘ë¦½", "ì‹ ë¢°ë„": "0.5000", "ëª¨ë“ _í™•ë¥ ": {"ë¶€ì •": "0.3333", "ì¤‘ë¦½": "0.3333", "ê¸ì •": "0.3333"}}
        
        # í† í°í™” ì‹œ ë” ì•ˆì „í•œ ì„¤ì •
        inputs = sentiment_tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=128,  # ê¸¸ì´ ì¦ê°€
            add_special_tokens=True
        )
        
        # GPUë¡œ ì…ë ¥ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = sentiment_model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            pred = torch.argmax(probs, dim=1).item()
            
        label_map = {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}
        return {
            "ì˜ˆì¸¡": label_map[pred],
            "ì‹ ë¢°ë„": f"{torch.max(probs).item():.4f}",
            "ëª¨ë“ _í™•ë¥ ": {label_map[i]: f"{probs[0][i].item():.4f}" for i in range(3)}
        }
    except Exception as e:
        print(f"ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"ì˜ˆì¸¡": "ì¤‘ë¦½", "ì‹ ë¢°ë„": "0.5000", "ëª¨ë“ _í™•ë¥ ": {"ë¶€ì •": "0.3333", "ì¤‘ë¦½": "0.3333", "ê¸ì •": "0.3333"}}

# === 2. í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ (KeyBERT + ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©) ===
print("í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë”© ì¤‘...")

# ë” ì•ˆì •ì ì¸ í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©
try:
    # ì²« ë²ˆì§¸ ì„ íƒ: jhgan/ko-sroberta-multitask
    kw_model = KeyBERT(model=SentenceTransformer("jhgan/ko-sroberta-multitask"))
    print("í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸: jhgan/ko-sroberta-multitask")
except Exception as e:
    try:
        # ë‘ ë²ˆì§¸ ì„ íƒ: paraphrase-multilingual-MiniLM-L12-v2
        kw_model = KeyBERT(model=SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2"))
        print("í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸: paraphrase-multilingual-MiniLM-L12-v2")
    except Exception as e2:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e2}")
        kw_model = None

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text, top_n=5):
    if kw_model is None:
        # ëª¨ë¸ì´ ì—†ì„ ê²½ìš° ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = text.split()
        return words[:top_n] if len(words) >= top_n else words
    
    try:
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if len(text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
            return text.split()
        
        keywords = kw_model.extract_keywords(
            text, 
            top_n=top_n, 
            stop_words=["ìˆë‹¤", "í–ˆë‹¤", "ëŒ€í•œ", "ë“±", "ì´ë‹¤", "í•˜ë‹¤", "ë˜ë‹¤", "ê²ƒ", "ìˆ˜", "ë•Œ", "ë…„", "ì›”", "ì¼"],
            use_mmr=True,  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ MMR ì‚¬ìš©
            diversity=0.5
        )
        return [kw for kw, score in keywords]
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ë‹¨ìˆœ ë¶„í• 
        words = text.split()
        return words[:top_n] if len(words) >= top_n else words

# === 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ ===
def analyze_news(news_text):
    print(f"ğŸ“° ì…ë ¥ ë‰´ìŠ¤: {news_text}\n")
    
    try:
        # 1. ì „ì²´ ë‰´ìŠ¤ ê°ì„± ë¶„ì„
        print("ğŸ“Š ì „ì²´ ë‰´ìŠ¤ ê°ì„± ë¶„ì„:")
        overall_sentiment = predict_sentiment(news_text)
        print(f"ì „ì²´ ê°ì„±: {overall_sentiment['ì˜ˆì¸¡']} (ì‹ ë¢°ë„: {overall_sentiment['ì‹ ë¢°ë„']})")
        print()
        
        # 2. í‚¤ì›Œë“œ ì¶”ì¶œ
        print("ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = extract_keywords(news_text)
        print("ì¶”ì¶œëœ í‚¤ì›Œë“œ:", keywords)
        print()
        
        # 3. ê° í‚¤ì›Œë“œ ê°ì„± ë¶„ì„
        if keywords:
            print("ğŸ“Š í‚¤ì›Œë“œë³„ ê°ì„± ë¶„ì„:")
            for kw in keywords:
                if kw.strip():  # ë¹ˆ í‚¤ì›Œë“œ ì œì™¸
                    result = predict_sentiment(kw)
                    print(f" â€¢ {kw}: {result['ì˜ˆì¸¡']} ({result['ì‹ ë¢°ë„']})")
        
        print("-" * 50)
        
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("-" * 50)

# === 4. ì•ˆì „í•œ í…ŒìŠ¤íŠ¸ ===
def safe_test():
    sample_news = [
        "ì •ë¶€ì˜ ë¶€ë™ì‚° ì •ì±…ì´ ì‹œì¥ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì£¼ê³  ìˆë‹¤.",
        "ë¬¼ê°€ ìƒìŠ¹ê³¼ ê¸ˆë¦¬ ì¸ìƒì´ ê²½ì œì— ë¶€ë‹´ì„ ì£¼ê³  ìˆë‹¤.",
        "ìƒˆë¡œìš´ ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ë¯¸ë˜ê°€ ë°ì•„ ë³´ì¸ë‹¤."
    ]
    
    for i, news in enumerate(sample_news, 1):
        print(f"=== í…ŒìŠ¤íŠ¸ {i} ===")
        try:
            analyze_news(news)
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {e}")
            print("-" * 50)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# === 5. ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    print("í•œêµ­ì–´ ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 50)
    safe_test()