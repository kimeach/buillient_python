import torch
from transformers import BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score
import numpy as np
import os
import json

# 1. í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "monologg/kobert"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)

# 2. í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
texts = [
    "ì½”ìŠ¤í”¼ê°€ ìƒìŠ¹í–ˆë‹¤.",
    "ê²½ì œ ì „ë§ì´ ë¶ˆíˆ¬ëª…í•˜ë‹¤.",
    "ì½”ìŠ¤ë‹¥ì€ ë³´í•©ì„¸ë¥¼ ë³´ì˜€ë‹¤."
]
labels = [2, 0, 1]  # 0=ë¶€ì •, 1=ì¤‘ë¦½, 2=ê¸ì •
dataset = Dataset.from_dict({'text': texts, 'label': labels})

def tokenize_fn(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=64)

tokenized_dataset = dataset.map(tokenize_fn, batched=True)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# 3. í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„
eval_texts = [
    "ê²½ì œì„±ì¥ë¥ ì´ ìƒìŠ¹í–ˆë‹¤.",
    "ë¬¼ê°€ê°€ í­ë“±í–ˆë‹¤.",
    "ê¸°ì—… ì‹¤ì ì´ ì˜ˆìƒë³´ë‹¤ ë‚˜ì˜ë‹¤."
]
eval_labels = [2, 0, 0]
eval_dataset = Dataset.from_dict({'text': eval_texts, 'label': eval_labels})
eval_dataset = eval_dataset.map(tokenize_fn, batched=True)
eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# 4. í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="weighted")
    return {'accuracy': acc, 'f1': f1}

# 5. í•™ìŠµ ì„¤ì • (save_strategyë¥¼ noë¡œ ì„¤ì •)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    warmup_steps=5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=5,
    evaluation_strategy="epoch",
    save_strategy="no"  # ìë™ ì €ì¥ ë¹„í™œì„±í™”
)

# 6. Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 7. í•™ìŠµ ì‹¤í–‰
trainer.train()

# 8. ìˆ˜ë™ ì €ì¥ (í† í¬ë‚˜ì´ì € ì €ì¥ ë¬¸ì œ í•´ê²°)
save_directory = "./best_model"
os.makedirs(save_directory, exist_ok=True)

# ëª¨ë¸ë§Œ ì €ì¥
model.save_pretrained(save_directory)

# í† í¬ë‚˜ì´ì € ì •ë³´ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì €ì¥ (ì›ë³¸ ëª¨ë¸ëª… ì €ì¥)
tokenizer_info = {
    "model_name": model_name,
    "trust_remote_code": True,
    "max_length": 64
}

with open(os.path.join(save_directory, "tokenizer_info.json"), "w", encoding="utf-8") as f:
    json.dump(tokenizer_info, f, ensure_ascii=False, indent=2)

print(f"âœ… ëª¨ë¸ì´ {save_directory}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("âš ï¸  í† í¬ë‚˜ì´ì €ëŠ” ì›ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.")

# 9. í‰ê°€ ì‹¤í–‰
eval_result = trainer.evaluate()
print("ğŸ“Š í‰ê°€ ê²°ê³¼:", eval_result)

# 10. ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_saved_model(save_directory):
    """ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜"""
    # í† í¬ë‚˜ì´ì € ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
    tokenizer_info_path = os.path.join(save_directory, "tokenizer_info.json")
    
    if os.path.exists(tokenizer_info_path):
        with open(tokenizer_info_path, "r", encoding="utf-8") as f:
            tokenizer_info = json.load(f)
        
        # ì›ë³¸ ëª¨ë¸ëª…ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
        loaded_tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_info["model_name"], 
            trust_remote_code=tokenizer_info["trust_remote_code"]
        )
    else:
        # í† í¬ë‚˜ì´ì € ì •ë³´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ëª¨ë¸ëª… ì‚¬ìš©
        loaded_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    loaded_model = BertForSequenceClassification.from_pretrained(save_directory)
    
    return loaded_tokenizer, loaded_model

# 11. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
loaded_tokenizer, loaded_model = load_saved_model(save_directory)

# 12. ì¶”ë¡  í•¨ìˆ˜ ì •ì˜
def predict(text, model=loaded_model, tokenizer=loaded_tokenizer):
    """ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜"""
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=64)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        pred = torch.argmax(probs, dim=1).item()
        confidence = torch.max(probs).item()
    
    label_map = {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}
    return {
        "ì˜ˆì¸¡": label_map[pred],
        "ì‹ ë¢°ë„": f"{confidence:.4f}",
        "ëª¨ë“ _í™•ë¥ ": {label_map[i]: f"{probs[0][i].item():.4f}" for i in range(3)}
    }

# 13. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
test_sentences = [
    "ì •ë¶€ì˜ ê²½ì œì •ì±…ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€ë°›ê³  ìˆë‹¤.",
    "ì£¼ì‹ì‹œì¥ì´ í¬ê²Œ í•˜ë½í–ˆë‹¤.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ íë¦¬ë‹¤."
]

print("\nğŸ” ì˜ˆì¸¡ ê²°ê³¼:")
for sentence in test_sentences:
    result = predict(sentence)
    print(f"ë¬¸ì¥: {sentence}")
    print(f"ê²°ê³¼: {result}")
    print("-" * 50)

# 14. ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
print(f"\nğŸ“ˆ ìµœì¢… í‰ê°€ ì„±ëŠ¥:")
print(f"ì •í™•ë„: {eval_result['eval_accuracy']:.4f}")
print(f"F1 ì ìˆ˜: {eval_result['eval_f1']:.4f}")
print(f"ì†ì‹¤: {eval_result['eval_loss']:.4f}")